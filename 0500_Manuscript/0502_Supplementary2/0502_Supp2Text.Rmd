---
title: '**S2 Statistical Methodology** for *Combining Rapid Antigen Testing and Syndromic Data Improves Sensitivity and Specificity in Real-World COVID-19 Detection*'
author:
  - name: Fergus J Chadwick
    email: 'f.chadwick.1@research.gla.ac.uk'
    affiliation: IBAHCM,UoGLMICS
  - name: Yacob Haddou
    email:  yacob.haddou@glasgow.ac.uk
    affiliation: IBAHCM,UoGLMICS
  - name: Tasnuva Chowdhury
    email:  tasnuvachowdhury2004@gmail.com
    affiliation: IBAHCM
  - name: David Pascall
    email:  david.pascall@mrc-bsu.cam.ac.uk
    affiliation: MRCB
  - name: Shayan Chowdhury
    email: shayan.chowdhury@a2i.gov.bd
    affiliation:  a2i
  - name: Jessica Clark
    email: Jessica.Clark@glasgow.ac.uk
    affiliation:  IBAHCM,UoGLMICS
  - name: Joanna Andrecka
    email: aandrecka@gmail.com
    affiliation: UNFAO
  - name: Mikolaj Kundergorski
    email: mikolaj.kundegorski@gmail.com
    affiliation: MathsandStatGla,UoGLMICS
  - name: Craig Wilkie
    email: craig.wilkie@glasgow.ac.uk
    affiliation: MathsandStatGla,UoGLMICS
  - name: Eric Brum
    email: eric.brum@fao.org
    affiliation: UNFAO
  - name: Tahmina Shirin 
    email: tahmina.shirin14@gmail.com
    affiliation: IEDCR
  - name: A S M Alamgir  
    email: aalamgir@gmail.com
    affiliation: IEDCR
  - name: Mahbubur Rahman
    email: dr_mahbub@yahoo.com
    affiliation: IEDCR
  - name: Ahmed Nawsher Alam
    email: anawsher@yahoo.com
    affiliation: IEDCR
  - name: Farzana Khan
    email: farzanakhan_25@yahoo.com
    affiliation: IEDCR
  - name: Janine Illian
    email: janine.illian@glasgow.ac.uk
    affiliation: MathsandStatGla,UoGLMICS
  - name: Ben Swallow
    email: ben.swallow@glasgow.ac.uk
    affiliation: MathsandStatGla,UoGLMICS
  - name: Davina L Hill
    email: davina.hill@glasgow.ac.uk
    affiliation: IBAHCM,UoGLMICS
  - name:  Dirk Husmeier
    email: dirk.husmeier@glasgow.ac.uk
    affiliation: MathsandStatGla
  - name: Jason Matthiopoulos
    email: jason.matthiopoulos@glasgow.ac.uk
    affiliation: IBAHCM,UoGLMICS
  - name: Katie Hampson
    email: katie.hampson@glasgow.ac.uk
    affiliation: IBAHCM,UoGLMICS
  - name: Ayesha Sania
    email: ays328@mail.harvard.edu
    affiliation: Columbia
address:
  - code: IBAHCM
    address: Institute of Biodiversity, Animal Health and Comparative Medicine, University of Glasgow
  - code: UoGLMICS
    address: 'COVID-19 in LMICs Research Group, University of Glasgow'
  - code: MRCB
    address: MRC Biostatistics Unit, University of Cambridge 
  - code: MathsandStatGla
    address: School of Mathematics and Statistics, University of Glasgow
  - code: a2i
    address: 'a2i, United Nations Development Program, ICT Ministry, Bangladesh' 
  - code: UNFAO
    address: UN FAO in support of the UN Interagency Support Team, Bangladesh
  - code: IEDCR
    address: Institute of Epidemiology, Disease Control and Research, Ministry of Health, Bangladesh
  - code: Columbia
    address: Division of Developmental Neuroscience, Department of Psychiatry, Columbia University
footnote:
  - code: 1
    text: "Corresponding Author"
journal: "Lancet: Global Health"
bibliography: Supp2.bib
linenumbers: true
numbersections: true
csl: elsevier-vancouver.csl
output:
  bookdown::pdf_book:
    base_format: rticles::elsevier_article
header-includes:
  - \renewenvironment{abstract}{}{}
  - \usepackage[colorinlistoftodos]{todonotes}
  - \newenvironment{nalign}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}\ignorespacesafterend}
---

Below we have extended the modelling description provided in the main text to include more technical detail.
The code used to implement these tasks is available at https://github.com/fergusjchadwick/COVID19_SyndromicRATDiagnosis.

```{r data-flowchart2, echo=FALSE, out.width='100%', fig.cap="Schematic description of identification of likely COVID-19 patients by community support teams (CSTs), swab collection and model definitions. The teams collected syndromic data (age, gender and presence/absence of 14 predetermined symptoms), and two sets of naso-pharyngeal swabs (one each for Rapid Antigen Testing and RT-PCR). We then used rapid antigen testing (RAT) and syndromic data, two imperfect but inexpensive diagnostics, to generate three model classes: RAT result only in Model Class 1, syndromic data only in Model Class 2, and both RAT result and syndromic data in Model Class 3. The RT-PCR test result is used to train and test each model using temporal cross-validation."}
knitr::include_graphics(paste0(getwd(),"/0500_Manuscript/0501_MainText/MainTextFigs/DataFlowchart.pdf"))
```

## Modelling

### Structure

We examined the ability of the two imperfect identification methods, syndromic modelling and RAT, to predict the patient's COVID-19 status when used separately and together.
These combinations define three model classes (Figure \@ref(fig:data-flowchart2)).

Model Class 1 uses only the RAT result.
It equates being RAT-positive with the patient being PCR-positive for COVID-19 (hereafter, PCR-positive), and being RAT-negative with PCR-negativity.
We can write this formally for the $i$th individual as:

\begin{nalign}
\text{PCR}_i &= \text{RAT}_i \\
\text{PCR} &\in\{0,1\} \\
\text{RAT} &\in\{0,1\}
\end{nalign}

Model Class 2 uses only the syndromic data.
For this model, we used a Bayesian multivariate probit model [@albert1993bayesian].
The multivariate probit structures the outcomes of the PCR test and symptoms presence/absence as a $D$-dimensional vector of binary outcomes ($\boldsymbol{y}_i=(y_{i1},y_{i2},\dots,y_{id})$, $y_{ij}\in\{0,1\}$).
These outcomes are determined by an indicator function which takes a $D$-dimensional vector of *continuous latent* variables ($\boldsymbol{z}_i=(z_{i1},z_{i2},\dots,z_{id})$, $z_{ij}\in\ \mathbb{R}$).
These latent continuous variables then covary as realisations of a $D$-dimensional multivariate normal, 
with the mean of the error structure informed by a linear predictor, $\sum_{j=1}^J x_{ij}\beta_{jd} + \epsilon_{id}$, and a covariance between dimensions, $\Sigma$.
The linear predictor allows us to condition the outcomes on risk factor variables (here, age and gender) while the covariance structure allows us to account for the correlated nature of the symptoms with each other and the outcome.
As Albert and Chib [@albert1993bayesian] identify in their paper, the covariance matrix formulation is not identifiable, with the variance, $diag (\Sigma)$ and means of the latent variables, $\boldsymbol{z}_i$ trading off against each other.
For this reason, we use a correlation matrix, $\Omega$, formulation with the variance set to 1.
A correlation based framework also makes communication with clinicians and other practitioners smoother as correlations are more familiar.

\begin{nalign}
y_{id} &= \mathbb{I}(z_{id} > 0) \\
\boldsymbol{z}_{i} &= \boldsymbol{x}_i \boldsymbol{\beta} + \boldsymbol{\epsilon}_{i} \\
z_{id} &= \sum_{j=1}^J x_{ij}\beta_{jd} + \epsilon_{id} \\
\boldsymbol{\epsilon}_i &\sim N(\boldsymbol{0}, \boldsymbol{\Omega}) \\
diag(\Omega)&=1 \\
{\beta} &\sim N(0,1) \\
\boldsymbol{\Omega} &\sim \text{LKJ}(1)
(\#eq:ModelClass2)
\end{nalign}


Model Class 3 combines the two data sources.
We utilise the specificity of RAT by treating RAT-positive patients as PCR-positive patients.
The RAT-negative patients are modelled using the sensitive syndromic approach using Model Class 2 to capture PCR-positive patients that are missed by the RAT.
This approach leverages the potential different syndromic profiles of PCR-positve patients who are RAT-positive and -negative, allowing the model to adapt solely to the latter.
Structurally, the model combines Equations \@ref(eq:ModelClass1) and \@ref(eq:ModelClass2), with RAT-positive patients being modelled using Equation \@ref(eq:ModelClass1), and RAT-negative patients with Equation \@ref(eq:ModelClass2).

By using a Bayesian formulation, we generate full posteriors for our parameter estimates, allowing natural quantification of uncertainty.
Bayesian methods also facilitate the use of more informative priors.
While we used minimally informative priors here (standard normals in the probit scale for betas and an LKJ correlation prior with minimal shrinkage, $\eta=1$ [@lewandowski2009generating]), more informative priors that incorporate spatio-temporal effects, for instance, would be natural extensions. 
The models were fitted to the data using Bayesian inference techniques based on Hamiltonian Monte Carlo in the Stan programming language [@carpenter2017stan].
The models all converged with zero divergent transitions and large effective sample sizes.

### Model Selection

We conducted backwards model selection (starting with the most complex, biologically plausible model) to identify a subset of models with the highest predictive power under temporal cross-validation (Figure \@ref(fig:modsel-flowchart2)).
For the cross-validation, we divided the data into 5 folds of equal sizes in time order (i.e. the first fold is formed of the chronologically first $\frac{N}{K}$ patients, where $N$ is the number of patients and $K$ is the number of folds, the second fold by the next $\frac{N}{K}$ etc.)
To test the sensitivity of this cross-validation structure, we also did a strict temporal division (i.e. the first $\frac{T}{K}$ days where $T$ is the number of days samples were taken on).
The results did not change qualitatively between these approaches. 

```{r modsel-flowchart2, echo=FALSE, out.width='100%', fig.cap="Schematic for rounds of model selection in the multivariate probit component of Model Classes 2 and 3. With 14 symptoms (only 5 shown here for demonstration purposes) and two covariates there are over 131000 possible model combinations. To make exploring these possible models computationally feasible and to reduce the risk of overfitting, we carried out two rounds of model selection. First, the data are divided into temporal cross-validation sets. The multivariate probit connects symptoms to the RT-PCR result through a correlation matrix. In the coarse model selection, the most complex feasible model (all symptoms and covariates) is fit to the training data. The estimated correlations between each symptom and the RT-PCR result are compared for each cross-validation set. The symptoms that have non-zero correlations in a systematic direction (i.e. all positively or all negatively correlated with RT-PCR result) are retained. The process is then repeated on each retained set of symptoms until the four symptoms in each model class with the strongest correlation to RT-PCR result. We then conduct a more exhaustive model selection on all the possible permutations of the four symptoms and two covariates. In this round, each model is fit to training data and used to predict for the test set, and the quality of those predictions is measured using cross-entropy scoring. The cross-entropy score is then used to select the best predictive model for each level of model complexity. Only these final models are then used for classification. This reduces the set of models tested as classifiers from >131 000 to just four per model class."}
knitr::include_graphics(paste0(getwd(),"/0500_Manuscript/0501_MainText/MainTextFigs/ModelSelectionFlowchart.pdf"))
```

The coarse round of model selection (Figure \@ref(fig:modsel-flowchart2)) selected candidate symptoms based on whether they had a strong and consistent correlation with PCR as estimated according to Equations \@ref(eq:ModelClass2) and \@ref(eq:ModelClass3).
The models were fit with both covariates throughout the coarse round and symptoms were compared in nested models.
In the fine round of model selection, these candidate symptoms and the covariate combinations (age and gender, age, gender and no covariates) were permuted to more exhaustively explore the model space.
Reducing the number of possible models using the two stages of model selection was necessary to reduce computational demand and reduce the risk of overfitting models to the test scenarios.
The large number of symptoms corresponds to a high number of potential model configurations (>131 000 for 14 symptoms and two covariates) which might perform well on the test sets (even under the challenging conditions of temporal cross-validation) but lack transferability.

By using general predictive power to narrow down the number of candidate models and then testing those models, we are more likely to choose models which generalise well to new data.
The number of candidate models used was not pre-determined but it was clear when fitting the models that there were "jumps" in performance (as defined below) between models containing five and four symptoms, so the models with one to four symptoms were used as the candidate models.
Zero symptom models were not included in the analysis as they do not correspond to a feasible policy (with covariates they would require governments to ask individuals of a given gender and age as COVID-19 positive, and without covariates they would involve randomly assigning individuals as COVID-19 positive).

### Predictive Performance

We scored the models' predictive power using binary cross-entropy (hereafter, cross-entropy).
Cross-entropy measures the accuracy of models that generate probabilities of binary outcomes, rather than make binary classifications, similar in concept to a mean square error for normally-distributed data, but adapted for binary data [@gneiting2007strictly].
A cross-entropy value close to zero corresponds to high levels of accuracy, with larger values indicating lower accuracy.
More specifically, the metric allows us to compare a binary vector, $\boldsymbol{y}\in [0,1]$, with a vector of probabilistic predictions ($p(\boldsymbol{y})\in (0,1)$) as follows:

\begin{nalign}
\boldsymbol{H}_p(q)=\frac{1}{N}\sum_{i=1}^{N}y_i\cdot log(p(y_i))+(1-y_i)\cdot log(1-p(y_i))
(\#eq:CrossEntropy)
\end{nalign}

The resulting score is comparable across all methods for assigning predictions where the same test data are used, allowing us to compare predictions from Model Classes 1-3.
$H_p(q)\in {0,\boldsymbol{R}_+}$ with zero indicating perfect prediction (assigning probabilities of ones and zeroes to outcomes of ones and zeros exactly) and larger values indicating worse predictions.

### Classification Performance

In applied settings, models must often be evaluated on their performance as classifiers rather than just as prediction engines (i.e. their ability to say a patient is COVID-19 positive or negative, not simply the probability the patient might be COVID-19 positive or negative).
To generate a classification, $\hat{Y}$, a probability threshold, $\hat{p}$, must be chosen over which patients are classified as COVID-19 positive:

\begin{nalign}
\hat{Y} &= \begin{cases}
    1,& \text{if } p(y) \geq \hat{p} \\
     0  & \text{otherwise}
\end{cases}
(\#eq:ClassificationThreshold)
\end{nalign}

Receiver operating characteristics (ROCs) are a way to measure the performance of a set of classifications in terms of true and false positives and negatives (TP, FP, TN and FN) and the rates of each of these classification types (e.g. $TPR = \frac{TP}{TP+FN}$, and $FPR = \frac{FP}{FP+TN}$).
The error rates are calculated with respect to a particular threshold, $\hat{p}$, or across the range of possible $\hat{p}$s to generate a ROC curve [@hoo2017roc].
In our epidemiological scenarios (outlined below) we use our ROC curve calculations to identify single thresholds which yield a required error rate.

We strongly emphasise that generic performance here is only used to show the flexibility of the model classes; the best model for a local situation can only be determined if the relative cost of false positives and false negatives is known.
Here, we choose three representative scenarios.
Each scenario has a requirement and error rate (defined in Table \@ref(tab:scenarios-tab2)).
We identify the threshold, $\hat{p}$, at which the requirement is most closely exceeded (i.e. if the requirement is an error rate should be a maximum 15%, the threshold that produces an error rate below 15% but as close to 15% as possible will be chosen).

In Scenario 1, we do not consider epidemiological context but simply minimise false negative and false positive rates equally.
We do this by maximising the two correct classification rates both individually and in total, as measured by the harmonic mean.
The harmonic mean is used widely in the classification literature as it is maximised by achieving large values in all its component parts, rather than the arithmetic mean which can be maximised by having one extremely large component at the expense of other components.
In other words, the arithmetic mean could be large because it has a very high TPR but a small TNR, whereas the harmonic mean will maximise both TPR and TNR.
While conceptually the harmonic mean is better suited than the arithmetic for this use case, both produce qualitatively the same results for these data. 

Scenario 2 corresponds to the current situation in Bangladesh at time of writing (July 2021), with COVID-19 cases beginning to rapidly increase again.
Under these circumstances, false negatives are extremely costly relative to false positives due to the exponential growth of the disease.

In Scenario 3, the pandemic is not declining but maintaining a steady rate of cases.
In this situation, policy-makers may be keen to keep false positive diagnoses low to prevent lockdown fatigue and to keep the workforce active.

The requirements in Scenario 2 and 3 were developed in discussion with the Institute of Epidemiology, Disease Control and Research (IEDCR), Bangladesh, for illustrative purposes.

```{r scenarios-tab2, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(kableExtra)
dt <- tibble(
   Scenarios = c("1 Agnostic", "2 Rising Cases", "3 Low-Level Cases"),
   Requirement = c("Maximise correct classification rates", 
                   "20% false negative rate", 
                   "20% false positive rate"),
   Error = c("Sum of error rates", "False positive rate", "False negative rate")
)
kable(dt, "latex", booktabs = T,
col.names = c("Scenario Name", 
              "Requirement", 
              "Performance Criterion (Error)"),
caption = "For each epidemiological scenario there is a requirement and a performance criterion.
The requirement refers to a base level of performance the model must achieve; in general this will be a maximum acceptable error rate of some kind.
These requirements were determined in discussion with members of the Institute of Epidemiology, Disease Control and Research, Ministry of Health, Bangladesh (IEDCR).
The requirement determines a probability threshold for each model which most closely exceeds that requirement (i.e. for a 20% requirement, 20.1% error is unacceptable even though it might be the closest achieve error rate to the requirement).
The performance criterion is then used to determine which model performs the 'best' given that the requirement has been met.") %>%
  kable_styling(full_width = TRUE)

```