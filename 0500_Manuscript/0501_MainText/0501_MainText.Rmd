---
title: Combining Rapid Antigen Testing and Syndromic Data Improves Sensitivity and Specificity in Real-World COVID-19 Detection
author:
  - name: Fergus J Chadwick
    email: 'f.chadwick.1@research.gla.ac.uk'
    affiliation: IBAHCM,UoGLMICS
  - name: Yacob Haddou
    email:  yacob.haddou@glasgow.ac.uk
    affiliation: IBAHCM,UoGLMICS
  - name: Tasnuva Chowdhury
    email:  tasnuvachowdhury2004@gmail.com
    affiliation: IBAHCM
  - name: David Pascall
    email:  david.pascall@mrc-bsu.cam.ac.uk
    affiliation: MRCB
  - name: Shayan Chowdhury
    email: shayan.chowdhury@a2i.gov.bd
    affiliation:  a2i
  - name: Jessica Clark
    email: Jessica.Clark@glasgow.ac.uk
    affiliation:  IBAHCM,UoGLMICS
  - name: Joanna Andrecka
    email: aandrecka@gmail.com
    affiliation: UNFAO
  - name: Mikolaj Kundergorski
    email: mikolaj.kundegorski@gmail.com
    affiliation: MathsandStatGla,UoGLMICS
  - name: Craig Wilkie
    email: craig.wilkie@glasgow.ac.uk
    affiliation: MathsandStatGla,UoGLMICS
  - name: Eric Brum
    email: eric.brum@fao.org
    affiliation: UNFAO
  - name: Tahmina Shirin 
    email: tahmina.shirin14@gmail.com
    affiliation: IEDCR
  - name: A S M Alamgir  
    email: aalamgir@gmail.com
    affiliation: IEDCR
  - name: Mahbubur Rahman
    email: dr_mahbub@yahoo.com
    affiliation: IEDCR
  - name: Ahmed Nawsher Alam
    email: anawsher@yahoo.com
    affiliation: IEDCR
  - name: Farzana Khan
    email: farzanakhan_25@yahoo.com
    affiliation: IEDCR
  - name: Janine Illian
    email: janine.illian@glasgow.ac.uk
    affiliation: MathsandStatGla,UoGLMICS
  - name: Ben Swallow
    email: ben.swallow@glasgow.ac.uk
    affiliation: MathsandStatGla,UoGLMICS
  - name: Davina L Hill
    email: davina.hill@glasgow.ac.uk
    affiliation: IBAHCM,UoGLMICS
  - name:  Dirk Husmeier
    email: dirk.husmeier@glasgow.ac.uk
    affiliation: MathsandStatGla
  - name: Jason Matthiopoulos
    email: jason.matthiopoulos@glasgow.ac.uk
    affiliation: IBAHCM,UoGLMICS
  - name: Katie Hampson
    email: katie.hampson@glasgow.ac.uk
    affiliation: IBAHCM,UoGLMICS
  - name: Ayesha Sania
    email: ays328@mail.harvard.edu
    affiliation: Columbia
address:
  - code: IBAHCM
    address: Institute of Biodiversity, Animal Health and Comparative Medicine, University of Glasgow
  - code: UoGLMICS
    address: 'COVID-19 in LMICs Research Group, University of Glasgow'
  - code: MRCB
    address: MRC Biostatistics Unit, University of Cambridge 
  - code: MathsandStatGla
    address: School of Mathematics and Statistics, University of Glasgow
  - code: a2i
    address: 'a2i Programme, ICT Ministry/UNDP Bangladesh' 
  - code: UNFAO
    address: UN FAO in support of the UN Interagency Support Team, Bangladesh
  - code: IEDCR
    address: Institute of Epidemiology, Disease Control and Research, Ministry of Health, Bangladesh
  - code: Columbia
    address: Division of Developmental Neuroscience, Department of Psychiatry, Columbia University
footnote:
  - code: 1
    text: "Corresponding Author"
journal: "Lancet: Global Health"
bibliography: mybibfile.bib
linenumbers: true
numbersections: true
csl: elsevier-vancouver.csl
output:
  bookdown::pdf_book:
    base_format: rticles::elsevier_article
header-includes:
  - \renewenvironment{abstract}{}{}
  - \usepackage[colorinlistoftodos]{todonotes}
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE, include = FALSE}
library(tidyverse)
library(kableExtra) 
library(viridis)
library(viridisLite)
pop_summ <- readRDS("0100_Data/0103_pop_summ.RDS")
pop_size <- sum(pop_summ$Gender_Count)
cross_entropy <- readRDS("0400_ModelAssessment/0430_MedCrossEnts.rds")
scenario_outcomes <- readRDS("0400_ModelAssessment/0420_scenario_outcomes.rds") %>% ungroup()

scenario_outcomes$scenario <- 
  recode(scenario_outcomes$scenario, `Agnostic` = "Scenario 1",
         `Costly False Negatives` = "Scenario 2",
         `Costly False Positives` = "Scenario 3") 
scenario_outcomes <- scenario_outcomes %>% 
  # Remove 0 symptom models as they aren't actionable
  filter(!FitType %in% c("0Symptom"))

RATonlyROC <- readRDS("0400_ModelAssessment/0410_RATOnly_ROCrate.rds")
RATonlyFPR <- RATonlyROC$MedFalsePosRate %>% round(2)
RATonlyFNR <- RATonlyROC$MedFalseNegRate %>% round(2)
```


# Abstract (Max 250 Words - Currently over)

*Background*

The majority of the world's population live in low- and middle-income countries (LMICs) where access to gold-standard diagnostics like RT-PCR is often limited.
Rapid antigen testing (RAT) and syndromic diagnosis are two alternative, inexpensive and easy-to-deploy surveillance methods but there are concerns that they lack the sensitivity and specificity to effectively guide practice.

*Methods*

Bangladesh's Institute of Epidemiology Disease Control And Research (IEDCR) identified potential COVID-19 patients in Dhaka using syndromic surveillance.
A sample (n = `r pop_size`) of these patients was tested using RAT and syndromic data were collected.
Models were fit to predict RT-PCR status using the RAT data, the syndromic data, and the two combined.
Model performance was measured using predictive power and classification performance under three epidemiological scenarios: "Agnostic", "Rising Cases" and "Low-Level Cases".

*Findings*

Combined data models yielded equal or improved performance over syndromic- and RAT-only models across all three epidemiological scenarios and when compared as more generic prediction and classification engines.
In the "Rising Cases" scenario, which most closely represents the current situation in many LMICs, the combined data model false negative rate is `r 100*(RATonlyFNR - scenario_outcomes %>% filter(scenario == "Scenario 2") %>% filter(ModelClass == "SyndRAT") %>% select(MedFalseNegRate) %>% unlist(use.names = FALSE) %>% min())` percentage points lower that of the RAT only model. 
Although the syndromic only model matches the combined models false negative rate, its false positive rate is `r 100*(scenario_outcomes %>% filter(scenario == "Scenario 2") %>% filter(ModelClass == "SyndOnly") %>% select(MedError) %>% unlist(use.names = FALSE) %>% min() - scenario_outcomes %>% filter(scenario == "Scenario 2") %>% filter(ModelClass == "SyndRAT") %>% select(MedError) %>% unlist(use.names = FALSE) %>% min())` percentage points higher. 

*Interpretation*

A few accurate tests may be less useful at the population level than many more imperfect ones. 
Small, scalable improvements in the accuracy of mass-deployed but imperfect tests can then make a very big difference for pandemic control.  
We demonstrate that such improvements can be achieved by statistically utilising complementary strengths and weaknesses across two imperfect diagnostics, we can greatly improve the detection of COVID-19.

*Funding*

The Bill and Melinda Gates Foundation and the Wellcome Trust.

# Introduction (~1107 Words)

Identification and isolation of COVID-19 cases remains key to the pandemic response across the globe.
The faster and more accurately we can identify cases, the more effectively we can provide clinical care, reduce transmission of infection and develop population-level interventions.
RT-PCR testing has rapidly become the default, gold-standard test for COVID-19 in applied settings due to its high sensitivity and specificity for COVID-19 [@corman2020detection, @tahamtan2020real].
Most of the world's population, however, live in low- and middle-income countries (LMICs) where the laboratory facilities needed to carry out RT-PCR tests are often scarce and hard to reach [@chowdhury2020long, @vandenberg2021considerations].
COVID-19 diagnosis worldwide, therefore, must be made accessible using inexpensive methods that can be carried out locally [@cash2020has, @olalekan2020covid].

An increasingly popular alternative to RT-PCR is rapid antigen testing (RAT) [@linares2020panbio].
Like RT-PCR, these tests have high specificity for COVID-19 while being less expensive, easier to implement, and faster to produce results [@boum2021performance].
RATs also require less commitment and discomfort for patients.
For RT-PCR testing, patients must travel to a designated site (such as a hospital or testing booth) or have highly visible PPE-clad officials visit their home.
Then, invasive nasopharyngeal swabs must be taken and there is a delay in receiving the result (between one day and a week in Bangladesh).
In contrast, RAT can be conducted on nasal or saliva samples, completed in the home with minimal PPE and results are available in 30 minutes.
RATs can be taken by persons with limited training, thus decreasing the time and expense associated with identifying cases.
Together, these traits make RATs an appealing alternative to RT-PCR.
However, several concerns have been raised about the sensitivity of RAT [@mak2020evaluation, muhi2021multi] leading to more false negative diagnoses.

Another alternative to RT-PCR, one that has been used since the start of the pandemic, is identifying cases through symptom-thresholding\todo{Not an official term} [@jin2020rapid].
In this approach, a patient presenting with a fever and one or more viral pneumonia symptoms is treated as a COVID-19 positive patient.
The main advantage of this approach is the ease of implementation. 
As with RAT the process is faster, cheaper and less invasive than RT-PCR, but unlike RAT the process relies on minimal equipment and thus can be scaled quickly and easily.
For example, in Bangladesh, an LMIC, much of the initial support and reporting of infections locally is provided by community support teams (CSTs) composed of local volunteers with basic training.
The CSTs can easily collect symptomatic data in the community and provide care where the thresholds are met.
However, these thresholds were developed early in the outbreak, and thus were necessarily drawn from clinical intuition, rather than data, and for different variants and populations than they are now applied to.
Consequently, the relationship between the thresholds and the true COVID-19 status is often weak, with low specificity leading to a very large number of false positive diagnoses.

A natural extension to these symptom-threshold approaches is syndromic modelling.
Here, a patient presenting with a fever and one or more viral pneumonia symptoms is treated as a potential COVID-19 patient.
However, rather than using a set of pre-determined criteria, a range of symptomatic and risk factor data are collected and then a sub-sample of patients is tested using RT-PCR for COVID-19 [@sim2021utilizing].
These data are used to fit a model that allows more accurate prediction of how likely a patient is to have COVID-19 through the identification of COVID-19 syndromes [@undurraga2021covid, @wenham2020covid].
It is worth highlighting at this point that in resource-limited settings, there is very limited provision for testing of asymptomatic cases, despite their important role in disease transmission [@mayorga2020modelling].
Even while focusing solely on symptomatic patients, syndromic modelling is a complex and nuanced task.
The strength of relationships between symptoms and diseases is not stable through time or across sampling strategies since the relative importance of each symptom for disease diagnosis, in part, depends on the prevalence of other diseases causing similar symptoms in the community [@garry2020considerations].
For example, if another disease for which loss of smell is a symptom becomes common, that symptom becomes a worse predictor for COVID-19.
Similarly, if everyone who presents has a cough and thus is included in the sample, then coughing will likely have a very low correlation with COVID-19 (even if the two are strongly related in the general population).
Symptoms are also inter-related, meaning that they cannot be interpreted independently.
The majority of methods used currently do not account for these changes through time, symptom-to-symptom correlations or the relationship between the population sampled and the target population. 
Even then, the many types of common respiratory disease generally means that even then these models tend to have relatively high false positive rates (low specificity) for COVID-19 [@garry2020considerations], although much lower than the symptom-threshold approach.

Poor sensitivity and specificity are problematic in diagnostics but higher error rates than gold-standard methods may be tolerable depending on their scale and impact given the local situation.
Low specificity means a large number of false positive classifications, where the patient is told they have COVID-19 but they actually do not.
This might lead to patients unnecessarily self-isolating and receiving support which can be expensive to the individuals and local public health bodies, as well as reducing available resources for those who need them [@surkova2020false].
Similarly, low sensitivity means more false negative classifications, where the patient is told they do not have COVID-19 but they actually do, which can lead to a health-risk for the individual and to the disease spreading further [@west2020covid].
The costs of these misclassifications will depend on local context.
When the prevalence of the disease is low, false positives may create local scepticism about the value of testing, or when there are strong population-level mitigations already in place (such as a nationwide lockdown), then false positives might be more costly than false negatives [@surkova2020false].
If the disease is abundant or increasing rapidly then false negatives are likely to be more costly.
In most situations, a balance will need to be struck [@vandenberg2021considerations].

The two dominant "alternative" testing methods available in resource limited settings, therefore, are both flawed.
Relying solely on symptomatic diagnosis will likely overestimate the number of individuals with COVID-19 due to its lack of specificity.
Conversely, RATs will give a false impression of control due to the number of positive cases that will be missed.
In this paper, we demonstrate how to combine these data types to exploit their complementarity and amelioriate their respective weaknesses.
We aim to compare the performance of these two testing methods and the combined approach both in terms of general prediction and as diagnostics under three epidemiological scenarios; and demonstrate that the combined data achieve equal to much lower error rates than the next best method.
We then discuss the role of statistically integrating data from multiple imperfect testing methods in resource limited settings to improve the diagnosis of diseases, particularly COVID-19.

# Methods (~1019 Words)

Participants included in this study were identified for COVID-19 testing after self-reporting symptoms to the Bangladesh government's national hotlines for COVID-19 support.
Recruitment took place across Dhaka (the capital city of Bangladesh) between 2nd April 2021 and 5th May 2021.

Patients were selected for further testing conditional on the presence of a fever (>38°C) at the point of testing and one or more of 14 additional symptoms associated with COVID-19 (breathing problems, coughing, diarrhoea, fever (ongoing), a headache, loss of taste, loss of smell, muscle pain, red eyes, a runny nose, a sore throat, tiredness, vomiting or a wet cough).
The patient's age and gender were also recorded, but these data were not included in the patient selection criteria.

Nasal swabs and syndromic data were collected from the patient by medical technologists.
One swab each was used for rapid antigen testing (RAT) and RT-PCR (gold-standard for COVID-19 status).
The full questionnaire and testing protocols are provided in Appendix XX. 
Participants provided written informed consent to sample collection and for their test results to be analyzed in the study.

```{r data-flowchart, echo=FALSE, out.width='100%', fig.cap="Schematic description of identification of likely COVID-19 patients by community support teams (CSTs), swab collection and model definitions. The teams collected syndromic data (age, gender and presence/absence of 14 predetermined symptoms), and two sets of naso-pharyngeal swabs (one each for Rapid Antigen Testing and RT-PCR). We then used rapid antigen testing (RAT) and syndromic data, two imperfect but inexpensive diagnostics, to generate three model classes: RAT result only in Model Class 1, syndromic data only in Model Class 2, and both RAT result and syndromic data in Model Class 3. The RT-PCR test result is used to train and test each model using temporal cross validation."}
knitr::include_graphics(paste0(getwd(),"/0500_Manuscript/0501_MainText/MainTextFigs/DataFlowchart.pdf"))
```

We examined the ability of the two imperfect identification methods, the syndromic profile and RAT result, to predict the patient's COVID-19 status when used separately and together.
The different data combinations define three model classes (Figure \@ref(fig:data-flowchart)).
\todo{Find more ways to cite Figure \@ref(fig:data-flowchart) in this text.}

Model Class 1 uses only the RAT result and is the simplest of the three.
It simply equates a positive RAT result with the patient being PCR positive, and a negative RAT result with PCR negativity.
Model Class 2 uses only the syndromic data and Model Class 3 combines the RAT result with the syndromic data.

For Model Class 2, we used a Bayesian multivariate probit model [@albert1993bayesian].
The multivariate probit structure allows the model to account for the correlations between, and binary nature of, the symptoms (e.g. loss of taste is often correlated with loss of smell).
By using a Bayesian formulation, we are able to better quantify the uncertainty in the parameter estimates.
Structurally, the multivariate probit model allows the symptoms and COVID-19 status to be treated as correlated binary outcomes with an intrinsic rate (the intercept for each variable) and the patient's age and gender, while propagating and quantifying uncertainty.

In Model Class 3, we model RAT positive patients as PCR positive and use the syndromic approach outlined for Model Class 2 for the RAT negative patients.
The models were fitted to the data using Hamiltonian Monte Carlo in the Stan programming language [@carpenter2017stan].

We conducted backwards model selection (starting with the most complex model feasible, with all 14 symptoms and both covariates) to identify a subset of models with the highest predictive power under temporal cross validation (Figure \@ref(fig:modsel-flowchart)).
Reducing the number of possible models to a small number of the most predictive models was necessary to reduce computational demand and reduce the risk of overfitting models to the test scenarios.
The large number of symptoms means that there is a high number of potential model configurations (>131 000 for 14 symptoms and two covariates) which might, by chance, perform  well on the test sets (even under the challenging conditions of temporal cross validation) but lack transferability.
By first using general predictive power to narrow down the number of candidate models and then testing those models under more specific scenarios, we are more likely to choose models which generalise well to new data.
The number of candidate models used was not pre-determined.
In fitting the models it became clear that there were "jumps" in performance (as defined below) between models containing five and four symptoms, so the models with zero to four symptoms were used as the candidate models.

We scored the models' predictive power using cross entropy.
Cross entropy measures the accuracy of probabilistic predictions for models that predict binary outcomes using probabilities [@gneiting2007strictly], similar in concept to a mean squared error.
A cross entropy value close to zero corresponds to high levels of accuracy, with larger values indicating lower accuracy.
As the score only uses the predicted probability and true values, it is possible to directly compare the predictions of any model for the same test set.
More details on the model structure and selection process, including code, are available in Appendix XX.


```{r modsel-flowchart, echo=FALSE, out.width='100%', fig.cap="Schematic for rounds of model selection in the multivariate probit component of Model Classes 2 and 3. With 14 symptoms (only 5 shown here for demonstration purposes) and two covariates there are over 131000 possible model combinations. To make exploring these possible models computationally feasible and, to reduce the risk of overfitting, we carried out two rounds of model selection. First, the data are divided into temporal cross validation sets. The multivariate probit connects symptoms to the RT-PCR result through a correlation matrix. In the coarse model selection, the most complex feasible model (all symptoms and covariates) is fit to the training data. The estimated correlations between each symptom and the RT-PCR result are compared for each cross validation set. The symptoms that have non-zero correlations in a systematic direction (i.e. all positively or all negatively correlated with RT-PCR result) are retained. The process is then repeated on each retained set of symptoms until the four symptoms in each model class with the strongest correlation to RT-PCR result. We then conduct a more exhaustive fine model selection on all the possible permutations of the four symptoms and two covariates. In this round, each model is fit to training data and used to predict for the test set, and the quality of those predictions is measured using cross entropy scoring. The cross entropy score is then used to select the best predictive model for each level of model complexity. Only these final models are then used for classification. This reduces the set of models tested as classifiers from >131 000 to just four per model class."}
knitr::include_graphics(paste0(getwd(),"/0500_Manuscript/0501_MainText/MainTextFigs/ModelSelectionFlowchart.pdf"))
```

We then compared models as classifiers using their false positive and false negative rates in three epidemiological scenarios.
In applied settings, models must often be evaluated on their performance as classifiers rather than just as prediction engines (i.e. their ability to say a patient is COVID-19 positive or negative, not simply the probability the patient might be COVID-19 positive or negative).
To generate a classification, a probability threshold must be chosen over which patients are classified as COVID-19 positive.

Classifier performance was compared using ROC curves and error rates under three epidemiological scenarios.
ROC curves show the true and false positive rates that each model can achieve.
To extract the error rate under the epidemiological scenarios (described below), we use the ROC calculations to identify the probability threshold which most closely meets the scenario requirement (see Table \@ref(tab:scenarios-tab).
Comparing specific scenarios allows classifier performance to be demonstrated in relevant scenarios.
Whether measuring classifier performance in specific scenarios or more generally, decisions need to be made about the relative cost and acceptable levels of the two types of misclassification (false positives and negatives).
We strongly emphasise that local context should be the guide in applying these methods.

In Scenario 1, we do not consider epidemiological context but simply weight false negative and false positive rates equally by aiming to maximise the overall correct classification rate.
Scenario 2 corresponds to the current situation in Bangladesh at time of writing (July 2021), with COVID-19 cases beginning to rapidly increase again.
Under these circumstances, false negatives are extremely costly relative to false positives due to the exponential growth of the disease.
In Scenario 3, the pandemic is not declining but maintaining a steady rate of cases.
In this situation, policy-makers may be keen to keep false positive diagnoses low to prevent lockdown fatigue and to keep the workforce active.

```{r scenarios-tab, echo = FALSE, message = FALSE, warning = FALSE}
dt <- tibble(
   Scenarios = c("1 Agnostic", "2 Rising Cases", "3 Low-Level Cases"),
   Requirement = c("Maximise correct classification rates", 
                   "Max. 20% false negative rate", 
                   "Max. 20% false positive rate"),
   Error = c("Sum of error rates", "False negative rate", "False positive rate ")
)
kable(dt, "latex", booktabs = T,
col.names = c("Scenario Name", 
              "Requirement", 
              "Performance Criterion (Error)"),
caption = "For each epidemiological scenario there is a requirement and a performance criterion.
The requirement refers to a base level of performance the model must achieve; in general this will be a maximum acceptable error rate of some kind.
The requirement determines a threshold for each model which most closely meets that requirement.
The performance criterion is then used to determine which model performs the 'best' given that the requirement has been met.") %>%
  kable_styling(full_width = TRUE)

```

# Results (~353 words)

A total of `r pop_size` subjects had data available for the current analyses.
The mean age of women participants (`r 100*pop_summ$Gender_Count[1]/pop_size %>% round()`% of the sample) was `r pop_summ$Age_Mean[1]  %>% round(2)` (SD = `r pop_summ$Age_SD[1] %>% round(2)`), and for men (`r round(100*pop_summ$Gender_Count[2]/pop_size)  %>% round()`% of the sample) was `r pop_summ$Age_Mean[2]  %>% round(2)` (SD = `r pop_summ$Age_SD[2] %>% round(2)`).
Participants were self-selecting and drawn from across Dhaka.

Model selection for Model Class 2 (syndromic data only) and 3 (syndromic and RAT data), each retained age as an explanatory variable and showed a marked decline in predictive power at more than 4 symptoms.
The final four symptoms in order of importance (i.e. the most important symptom was retained in all of the final 4 models, the least important symptom was only retained in the 4 symptom model) were wet cough, runny nose, loss of smell and breathing problems for Model Class 2, and fever, wet cough, tiredness and diarrhoea for Model Class 3.
For both Model Class 2 and Model Class 3 model selection retained age as a covariate but not gender.

In the comparison of model predictive performance, Model Class 1 (RAT only) performed worst with a cross entropy of  `r cross_entropy %>% filter(ModelClass == "RATonly") %>% select(MedLogLoss) %>% unlist(use.names = FALSE) %>% round(2)` (cross entropy values further from zero correspond to worse predictive performance).
The median cross entropy values were between `r cross_entropy %>% filter(ModelClass == "SyndOnly") %>% select(MedLogLoss) %>% unlist(use.names = FALSE) %>% min() %>% round(2)` and `r cross_entropy %>% filter(ModelClass == "SyndOnly") %>% select(MedLogLoss) %>% unlist(use.names = FALSE) %>% max() %>% round(2)` for models in Class 2 (syndromic data only).
Models in Class 3 (combined data model) performed best with cross entropy values between `r cross_entropy %>% filter(ModelClass == "SyndRAT") %>% select(MedLogLoss) %>% unlist(use.names = FALSE) %>% min() %>% round(2)` and `r cross_entropy %>% filter(ModelClass == "SyndRAT") %>% select(MedLogLoss) %>% unlist(use.names = FALSE) %>% max() %>% round(2)` (see Figure \@ref(fig:pred-perf)).

```{r pred-perf, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Predictive performance of candidate models. Interquartile ranges for the posterior cross entropy of the best candidate models at each level of model complexity tested under temporal cross-validation. Cross entropy is a measure of distance from the truth, so values closer to zero indicate better models. The intermediate complexity models perform best at prediction, although performance is similar across all the models within each model class (1: rapid antigen testing (RAT) only; 2: syndromic data only; and 3: combined RAT and syndromic data)."}

# Load libraries and helper functions
source("0000_HelperCode_Libraries/0001_Libraries.R")
source("0000_HelperCode_Libraries/0003_HelperFunctions.R")

# Read in dataframe of best models - indexing to remove zero symptom models as they have no
# practical implementation
best_models_syndonly <- readRDS("0300_ModelSelection/0310_SyndromicOnly_BestModels.rds")
best_models_syndrat <- readRDS("0300_ModelSelection/0310_SyndromicRAT_BestModels.rds")

# rds files for best models
best_model_files_syndonly <- paste0("0300_ModelSelection/Output/SyndromicOnly_Fine_Round", 
                                    best_models_syndonly$ModelName, ".rds")
best_model_files_syndrat  <- paste0("0300_ModelSelection/Output/SyndromicRAT_Fine_Round",
                           best_models_syndrat $ModelName, ".rds")

## Read each file in parallel, extract log loss df, bind into single data frame,
## calculate model level log loss
# Syndromic only models
best_valid_syndonly <- best_model_files_syndonly %>% 
  future_map_dfr(read_logloss) %>% 
  pivot_wider(names_from = CV, values_from = log_loss, values_fn = list) %>%
  unnest(c(`1`, `2` , `3`, `4`,  `5`))  
best_valid_syndonly <- best_valid_syndonly %>%  
  mutate(ModelLogLoss = best_valid_syndonly %>%
           select(c(`1`, `2` , `3`,  `4`, `5`)) %>%
           rowSums())
best_valid_syndonly$ModelClass  <- "SyndOnly"
# Syndromic plus RAT models
best_valid_syndrat <- best_model_files_syndrat %>% 
  future_map_dfr(read_logloss)%>% 
  pivot_wider(names_from = CV, values_from = log_loss, values_fn = list) %>%
  unnest(c(`1`, `2` , `3`, `4`,  `5`))  
best_valid_syndrat <- best_valid_syndrat %>%  
  mutate(ModelLogLoss = best_valid_syndrat %>%
           select(c(`1`, `2` , `3`,  `4`, `5`)) %>%
           rowSums())
best_valid_syndrat$ModelClass  <- "SyndRAT"
# RAT only model
RATonly <- readRDS("0100_Data/0103_nasal_dat.RDS")
best_valid_RATonly <- data.frame("SwabType" = "nasal",
                                 "FitType" = "RATonly",
                                 "log_loss" = NA,
                                 "ModelLogLoss" = 
                                   LogLoss(y_pred = RATonly$nasal_ag,
                                           y_true = RATonly$result),
                                 "Iter" = NA,
                                 "Chain" = NA,
                                 "ModelClass" = "RATonly"
)

# Bind into one data frame  
best_valid <- rbind(best_valid_syndonly %>% 
                      select(FitType, ModelLogLoss, ModelClass),
                    best_valid_syndrat %>% 
                      select(FitType, ModelLogLoss, ModelClass),
                    best_valid_RATonly %>% 
                      select(FitType, ModelLogLoss, ModelClass)) 
# Tidy names
best_valid$FitType <- paste(parse_number(best_valid$FitType), 
                              "Symptom(s)", sep = " ")
best_valid$FitType[20001] <- "RATonly"
best_valid$SympNum <- paste0(parse_number(best_valid$FitType), " Symptoms")
best_valid$SympNum[20001] <- "RAT only"
# best_valid$ModStru <- best_valid$FitType
best_valid$FitType <- paste(best_valid$ModelClass, best_valid$FitType, sep = "_")

model_classes <- c(
                    `RATonly` = "Model Class 1",
                    `SyndOnly` = "Model Class 2",
                    `SyndRAT` = "Model Class 3"
                    )

# Plot
# Remove 0 Symptom models as they are not actionable
ggplot(best_valid %>% filter(SympNum != "0 Symptoms"), 
       aes(x = ModelLogLoss, y = as.factor(SympNum), colour = FitType)) +
  geom_boxplot() +
  coord_cartesian(xlim = c(0,3.5)) + 
  ylab("Candidate Model") +
  xlab("Cross Entropy") +
  facet_grid(rows = vars(ModelClass), 
             scales = "free_y", 
             labeller = as_labeller(model_classes)) +
  theme_minimal() + 
  guides(colour = FALSE)

```

General model classification performance is shown by the full ROC curves for each model (Figure \@ref(fig:ROC-plot)).

Scenario specific classification performance is shown in Figure \@ref(fig:scenario-plot).
In Scenario 1 ("Agnostic", see Table \@ref(tab:scenarios-tab)), the median error was `r scenario_outcomes %>% filter(scenario == "Scenario 1") %>% filter(ModelClass == "RATonly") %>% select(MedError) %>% unlist(use.names = FALSE)` for models in Class 1 and Class 3 and between `r scenario_outcomes %>% filter(scenario == "Scenario 1") %>% filter(ModelClass == "SyndOnly") %>% select(MedError) %>% unlist(use.names = FALSE) %>% min()` and `r scenario_outcomes %>% filter(scenario == "Scenario 1") %>% filter(ModelClass == "SyndOnly") %>% select(MedError) %>% unlist(use.names = FALSE) %>% max()` for models in Class 2 (Figure \@ref(fig:scenario-plot)).
In Scenario 2 ("Rising Cases"), Model Class 1 was unable to meet the required false negative rate.
The median errors were between 
`r scenario_outcomes %>% filter(scenario == "Scenario 2") %>% filter(ModelClass == "SyndOnly") %>% select(MedError) %>% unlist(use.names = FALSE) %>% min()` and 
`r scenario_outcomes %>% filter(scenario == "Scenario 2") %>% filter(ModelClass == "SyndOnly") %>% select(MedError) %>% unlist(use.names = FALSE) %>% max()` for models in Class 2, and 
`r scenario_outcomes %>% filter(scenario == "Scenario 2") %>% filter(ModelClass == "SyndRAT") %>% select(MedError) %>% unlist(use.names = FALSE) %>% min()` and 
`r scenario_outcomes %>% filter(scenario == "Scenario 2") %>% filter(ModelClass == "SyndRAT") %>% select(MedError) %>% unlist(use.names = FALSE) %>% max()` for models in Class 3 (Figure \@ref(fig:scenario-plot)).
In Scenario 3 ("Low-Level Cases"), the error in Class 1 was `r scenario_outcomes %>% filter(scenario == "Scenario 3") %>% filter(ModelClass == "RATonly") %>% select(MedFalsePosRate) %>% unlist(use.names = FALSE) %>% unique()` and the median errors ranged from `r scenario_outcomes %>% filter(scenario == "Scenario 3") %>% filter(ModelClass == "SyndOnly") %>% select(MedFalsePosRate) %>% unlist(use.names = FALSE) %>% min()` to `r scenario_outcomes %>% filter(scenario == "Scenario 3") %>% filter(ModelClass == "SyndOnly") %>% select(MedFalsePosRate) %>% unlist(use.names = FALSE) %>% max()` for Class 2, and `r scenario_outcomes %>% filter(scenario == "Scenario 3") %>% filter(ModelClass == "SyndRAT") %>% select(MedFalsePosRate) %>% unlist(use.names = FALSE) %>% min()` to `r scenario_outcomes %>% filter(scenario == "Scenario 3") %>% filter(ModelClass == "SyndRAT") %>% select(MedFalsePosRate) %>% unlist(use.names = FALSE) %>% max()` for Class 3 (Figure \@ref(fig:scenario-plot)).

```{r ROC-plot, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = " Receiver operating characteristics for rapid antigen testing (RAT) only approach (Model Class 1) and posterior mean  (+- posterior standard deviation) receiver operating characteristics for Class 2 (syndromic data only) and 3 (syndromic and RAT data) models. These curves demonstrate the performance of the model for any hypothetical scenario as defined by the axes (as opposed to Figure 5 which demonstrates model performance in specific epidemiological scenarios which are realisations of a single point in this space)."}
ROC_plots <- readRDS("0400_ModelAssessment/0420_ROC_plot_dat.rds")
model_classes <- c(
                    `RATonly` = "RAT Only",
                    `SyndOnly` = "Syndromic Only",
                    `SyndRAT` = "Combined"
                    )

ggplot(ROC_plots%>% filter(FitType!="0Symptom"), 
       aes(x = MedFalsePosRate, y = MedTruePosRate, 
                     colour = FitType)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = MedTruePosRate - SDTruePosRate, 
                    ymax = MedTruePosRate + SDTruePosRate)) +
  
  geom_errorbarh(aes(xmin = MedFalsePosRate - SDFalsePosRate, 
                     xmax = MedFalsePosRate + SDFalsePosRate)) +
  geom_line() +
  geom_abline(slope = 1, alpha = 0.1) +
  ylab("True Positive Rate") +
  xlab("False Positive Rate") +
  facet_wrap(~ModelClass, labeller = as_labeller(model_classes)) +
  theme_minimal() %+replace% 
  theme(legend.position = "bottom", aspect.ratio = 1)
  


```

```{r scenario-plot, echo=FALSE, fig.cap="Performance of models under each scenario measured by errors defined in Table 2. Low errors correspond to better model performance. There is no error rate defined for the Model Class 1 (RAT only model) in Scenario 2 as the model failed to meet the requirement for that scenario (making the error functionally infinite).", message=FALSE, warning=FALSE}

ggplot(scenario_outcomes, 
       aes(x = MedError, y = FitType, colour = ModelClass)) +
  geom_point() +
  geom_errorbarh(aes(xmin = MedError - SDError, 
                    xmax = MedError + SDError)) +
  coord_cartesian(xlim = c(0,1.2)) +
  ylab("Model Structure") + 
  xlab("Error") +
  facet_wrap(~scenario)  +
  scale_colour_discrete(name = "Model Class", 
                        labels = c("RAT only", 
                                   "Syndromic Only", 
                                   "Combined")) +
  theme_minimal() %+replace% 
  theme(legend.position = "bottom", aspect.ratio = 1, 
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
  
  

```

# Discussion (~1314 Words)

We have demonstrated that combining rapid antigen tests (RATs) with syndromic modelling yields better prediction of COVID-19 status and greater flexibility than each diagnostic individually. 
These improvements are non-trivial in real-world settings.
In Bangladesh, there are currently 15 000 new cases being identified every day, using only the limited supply of RT-PCR, the pandemic growth is accelerating and every missed case has a compounding effect. 
Scenario 2 ("Rising Cases") was developed with the need to keep false negative rates low and maps well onto the situation in Bangladesh (see Table \@ref(tab:scenarios-tab)).
In this scenario, the combined data model (Model Class 3) false negative rate is 
`r 100*(RATonlyFNR - scenario_outcomes %>% filter(scenario == "Scenario 2") %>% filter(ModelClass == "SyndRAT") %>% select(MedFalseNegRate) %>% unlist(use.names = FALSE) %>% min())` percentage points lower that of the RAT only model (Model Class 1). 
Although the syndromic only model (Model Class 2) matches the combined models false negative rate, its false positive rate is
`r 100*(scenario_outcomes %>% filter(scenario == "Scenario 2") %>% filter(ModelClass == "SyndOnly") %>% select(MedError) %>% unlist(use.names = FALSE) %>% min() - scenario_outcomes %>% filter(scenario == "Scenario 2") %>% filter(ModelClass == "SyndRAT") %>% select(MedError) %>% unlist(use.names = FALSE) %>% min())` percentage points higher.
These are large performance gains for any diagnostic but when deployed at the scale of Bangladesh and similar countries, these improvements represent catching tens of thousands of cases that would otherwise be missed. 
Furthermore, this boost is achieved with data that are already being collected in Bangladesh and other low- and middle- income countries (LMICs). 
Outwith developing and rerunning the models presented in this paper, these improvements are essentially cost free and eminently scalable.

The pattern is similar in epidemiological Scenarios 1 ("Agnostic") and 3 ("Low-Level Cases"), with the combined model class performing performing equally well or better than the other two classes (Figure \@ref(fig:scenario-plot)).
These three scenarios only offer snapshots of performance, however, and we strongly advocate defining model performance in terms of false negative and false positive rates with reference to local conditions.
An indication of how these models will perform under any condition can be obtained by comparing the more generic model performance metrics for prediction and classification (Figures \@ref(fig:pred-perf) and \@ref(fig:ROC-plot), respectively).
These figures demonstrate both the added flexibility of the more complex model classes that allow them to be tailored to specific needs and the need to combine the high-quality but inflexible RAT results with the more flexible but lower quality syndromic data.
Interestingly, the most of the Class 2 models performs approximately as well as chance except the simplest which performs worse than chance.
A model that performs worse than random can still be useful if one takes the inverse decision. \todo{@Dirk - can you please clarify this or suggest reasons the model is performing so badly. The red line here is a univariate probit regression with one continuous covariate so I don't understand why it's performing so poorly unless the temporal cross validation sets are wildly different from each other?}
Even a flexible model which performs as well as random classification can be useful if those error rates reflect those needed in a given local situation.
Fortunately, Model Class 3 is both flexible and performs better than random. 

We have deliberately not emphasised the final symptoms chosen through model selection in this paper as we are focusing on prediction and classification for a unique sub-population: self-referring, symptomatic patients. 
We do, however, highlight that while fever and loss of smell were the two most important symptoms in the two classes of syndromic models,
the other symptoms retained were different (with cough and wet cough retained in the combined syndromic and RAT model, Class 3, and loss of taste and vomiting in the syndromic only model, Class 2).
Further research is needed to understand the mechanisms by which symptoms predict COVID-19 and by which RAT misses COVID-19. 
Of particular interest is whether individuals that are missed by RAT are less infectious, which could be explored by using Threshold Cycle (Ct) values from the RT-PCR to compare viral load with respect to prediction by the different methods [@albert2021field]. 
We note also that, as expected, age was retained in model selection. 
We were, however, surprised that gender was removed during model selection.
Gender is thought to play a major role in infection risk [@wenham2020covid, @menni2020real, @wolff2020risk]. 
As we are looking to predict symptomatic COVID-19 in symptomatic individuals, generalised risk of infection is perhaps less predictive than expected, potentially due to the balancing of risk and burden [@joe2020equal]. 

Using a large sample collected under field-realistic conditions, we have rigorously tested our approach.
By taking a statistical modelling approach to case identification, we are able to update our diagnostic process in real time, allowing this method to readily adapt to new variants (or even new diseases) or new priorities for resource allocation. 
The modelling frameworks we have used are also sufficiently flexible to accommodate new data sources. 
Of particular interest are extensions to include the “pandemic context” in the model using space-time data. 
Furthermore, by using more sophisticated modelling structures that work at the scale of probabilities, rather than binary tests, it is possible to tune error rates to better reflect the local relative costs of false positives and false negatives.
Naturally, these strengths have complementary limitations. 
Our models require updating in real-time and can only achieve good performance if the validation data are of high quality. 
Similarly, targeting error rates is only sensible if those rates properly reflect local conditions which is hard to do in practice. 
These limitations should be seriously considered but the alternatives for imperfect testing methods are diagnostics that cannot be tailored to local conditions at all (and, as such may perform worse than a method which is sub-optimally tailored to local conditions) or diagnostics which make these decisions implicitly and not explicitly. 
We choose to make these decisions explicitly to allow them to be more readily challenged, researched and improved upon. 
We also emphasise the need for rigorous experimental design to ensure findings from the sample population are applicable to the target population and the need for further research into understanding error rate trade-offs in applied settings. 

We believe that the combined syndromic and rapid antigen testing approach represents the most promising approach to large-scale testing in LMICs for COVID-19 at present. 
By using the small amount of RT-PCR testing possible and formally integrating multiple imperfect, non-gold-standard methods, we can tune these diagnostics to our local conditions. 
We have demonstrated that these improvements can be impressive in real-world scenarios, and will have a large impact when scaled to the population sizes in LMICs. 
The methodology we have outlined here is applicable to a wide range of diseases and settings across LMICs. 
One of the biggest challenges in diagnosing and tracking many diseases in resource-limited settings is the low availability of access to gold-standard testing (such as RT-PCR in the case of COVID-19) and high error rates of alternative testing methods. 
In this paper, we have outlined the process for coupling a small number of gold-standard tests with formal statistical integration of alternative testing methods, to generate high quality diagnostic models. 
This process readily maps onto many other case identification problems, including the diagnosis of several neglected tropical diseases. For example, malaria (gold standard (GS) is also RT-PCR, imperfect methods (IM) include antigen tests, syndromic diagnosis and blood smears), schistosomiasis (GS: RT-PCR or autopsy; IM: Kato Katz egg counts, antibody detection) and rabies (GS: fluorescent antibody test; IM: light microscopy, differential diagnosis).

The management of global pandemics can only be done with global testing.
While the quest to achieve this using only gold-standard diagnostic methods is laudable, it is also often impractical.
Imperfect diagnostics are frequently imperfect in different ways, and these differences are ripe for statistical exploitation.
What is more, these approaches are often more agile than gold-standard diagnostics in situations of flux, for example, in the early stages of new pandemics or disease strains, when fast responses are essential.  
By investing in understanding how to utilise the complementary strengths of imperfect testing and deploy the limited gold-standard testing available for validation, we can provide good quality testing at the scale needed to fight infectious diseases.

# Funding (~26 words)

The Bill and Melinda Gates Foundation funded work by FAO (INV-022851), and University of Glasgow reports funding from Wellcome (207569/Z/17/Z). 
The authors declare no competing interests.

# Acknowledgements (~67 words)

We would like to thank members of the community support teams in Bangladesh who have provided essential services throughout the pandemic.
Earlier drafts of this mansucript benefited from the input of Daniel Haydon, Anne-Sophie Bonnet-Lebrun, Luca Nelli, Crinan Jarrett, Rita Claudia Cardoso Ribeiro, Halfan Ngowo, Heather McDevitt and Gina Bertolacci.
The University of Glasgow COVID-19 in LMICs Group provided the environment in which to develop this work.

# References (Max 30)
