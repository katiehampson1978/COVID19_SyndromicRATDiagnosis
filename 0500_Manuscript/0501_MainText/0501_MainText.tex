\documentclass[]{elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
\usepackage[hyphens]{url}

  \journal{Lancet: Global Health} % Sets Journal name


\usepackage{lineno} % add
  \linenumbers % turns line numbering on
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\usepackage{graphicx}
\usepackage{booktabs} % book-quality tables
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\bibliographystyle{elsarticle-harv}
\usepackage{longtable}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Combining Rapid Antigen Testing and Syndromic Data Improves Sensitivity and Specificity in Real-World COVID-19 Detection},
            colorlinks=false,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

\setcounter{secnumdepth}{5}
% Pandoc toggle for numbering sections (defaults to be off)

% Pandoc citation processing
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
% for Pandoc 2.8 to 2.10.1
\newenvironment{cslreferences}%
  {}%
  {\par}
% For Pandoc 2.11+
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% Pandoc header
\renewenvironment{abstract}{}{}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}



\begin{document}
\begin{frontmatter}

  \title{Combining Rapid Antigen Testing and Syndromic Data Improves Sensitivity and Specificity in Real-World COVID-19 Detection}
    \author[IBAHCM,UoGLMICS]{Fergus J Chadwick}
   \ead{f.chadwick.1@research.gla.ac.uk} 
    \author[IBAHCM,UoGLMICS]{Yacob Haddou}
   \ead{yacob.haddou@glasgow.ac.uk} 
    \author[IBAHCM]{Tasnuva Chowdhury}
   \ead{tasnuvachowdhury2004@gmail.com} 
    \author[MRCB]{David Pascall}
   \ead{david.pascall@mrc-bsu.cam.ac.uk} 
    \author[a2i]{Shayan Chowdhury}
   \ead{shayan.chowdhury@a2i.gov.bd} 
    \author[IBAHCM,UoGLMICS]{Jessica Clark}
   \ead{Jessica.Clark@glasgow.ac.uk} 
    \author[UNFAO]{Joanna Andrecka}
   \ead{aandrecka@gmail.com} 
    \author[MathsandStatGla,UoGLMICS]{Mikolaj Kundergorski}
   \ead{mikolaj.kundegorski@gmail.com} 
    \author[MathsandStatGla,UoGLMICS]{Craig Wilkie}
   \ead{craig.wilkie@glasgow.ac.uk} 
    \author[UNFAO]{Eric Brum}
   \ead{eric.brum@fao.org} 
    \author[IEDCR]{Tahmina Shirin}
   \ead{tahmina.shirin14@gmail.com} 
    \author[IEDCR]{A S M Alamgir}
   \ead{aalamgir@gmail.com} 
    \author[IEDCR]{Mahbubur Rahman}
   \ead{dr\_mahbub@yahoo.com} 
    \author[IEDCR]{Ahmed Nawsher Alam}
   \ead{anawsher@yahoo.com} 
    \author[IEDCR]{Farzana Khan}
   \ead{farzanakhan\_25@yahoo.com} 
    \author[MathsandStatGla,UoGLMICS]{Janine Illian}
   \ead{janine.illian@glasgow.ac.uk} 
    \author[MathsandStatGla,UoGLMICS]{Ben Swallow}
   \ead{ben.swallow@glasgow.ac.uk} 
    \author[IBAHCM,UoGLMICS]{Davina L Hill}
   \ead{davina.hill@glasgow.ac.uk} 
    \author[MathsandStatGla]{Dirk Husmeier}
   \ead{dirk.husmeier@glasgow.ac.uk} 
    \author[IBAHCM,UoGLMICS]{Jason Matthiopoulos}
   \ead{jason.matthiopoulos@glasgow.ac.uk} 
    \author[IBAHCM,UoGLMICS]{Katie Hampson}
   \ead{katie.hampson@glasgow.ac.uk} 
    \author[Columbia]{Ayesha Sania}
   \ead{ays328@mail.harvard.edu} 
      \address[IBAHCM]{Institute of Biodiversity, Animal Health and Comparative Medicine, University of Glasgow}
    \address[UoGLMICS]{COVID-19 in LMICs Research Group, University of Glasgow}
    \address[MRCB]{MRC Biostatistics Unit, University of Cambridge}
    \address[MathsandStatGla]{School of Mathematics and Statistics, University of Glasgow}
    \address[a2i]{a2i, United Nations Development Program, ICT Ministry, Bangladesh}
    \address[UNFAO]{UN FAO in support of the UN Interagency Support Team, Bangladesh}
    \address[IEDCR]{Institute of Epidemiology, Disease Control and Research, Ministry of Health, Bangladesh}
    \address[Columbia]{Division of Developmental Neuroscience, Department of Psychiatry, Columbia University}
      \cortext[1]{Corresponding Author}
  
  \begin{abstract}
  
  \end{abstract}
  
 \end{frontmatter}

\hypertarget{abstract-252-words}{%
\section{Abstract (252 words)}\label{abstract-252-words}}

\emph{Background}

The majority of the world's population live in low- and middle-income countries (LMICs), where access to gold-standard diagnostics like polymerase chain reaction (PCR) is limited.
Rapid antigen testing (RAT) and syndromic diagnosis are two alternative, inexpensive and easy-to-deploy surveillance methods but there are concerns that they lack the sensitivity and specificity to effectively guide practice.

\emph{Methods}

Community support teams in Dhaka, Bangladesh, identified potential COVID-19 patients using syndromic surveillance.
A sample (n = \text{1172}) of these patients was tested using PCR and RAT, and syndromic data were collected.
Statistical models were fit to predict PCR status using only the RAT data, only the syndromic data, and both combined.
Model performance was measured using predictive power and classification performance under three real-world, epidemiological scenarios: ``Agnostic,'' ``Rising Cases'' and ``Low-Level Cases.''

\emph{Findings}

Combined data models yielded equal or improved performance over syndromic- and RAT-only models across all three scenarios and by scenario-free prediction and classification metrics.\\
In the ``Rising Cases'' scenario, which most closely represents the current situation in many LMICs, the combined data model's false negative rate is \text{26} (IQR: \text{24}-\text{29}) percentage points lower than the RAT only model's.

\emph{Interpretation}

We demonstrate that by statistically utilising complementary strengths and weaknesses across two imperfect diagnostics, we can greatly improve the detection of COVID-19.
Small, scalable improvements in the accuracy of already mass-deployed but imperfect diagnostic methods can therefore make a big difference for pandemic control.

\emph{Funding}

The Bill and Melinda Gates Foundation, the Wellcome Trust and EPSRC.

\hypertarget{introduction-1079-words}{%
\section{Introduction (1079 Words)}\label{introduction-1079-words}}

Identification and isolation of COVID-19 cases remains key to the global pandemic response.
The faster and more accurately we can identify cases, the more effectively we can provide clinical care, reduce transmission of infection and develop population-level interventions.
Reverse transcription polymerase chain reaction (RT-PCR, hereafter, PCR) testing has rapidly become the default, gold-standard test for COVID-19 in applied settings (although see {[}1{]}) due to its high sensitivity and specificity for COVID-19 {[}3{]}.
Most of the world's population, however, live in low- and middle-income countries (LMICs), where the laboratory facilities needed to carry out PCR tests are often scarce and hard to access {[}5{]}.
In such settings, patient diagnosis and support comes from telemedicine or community support teams (CSTs) composed of local volunteers with basic training.
COVID-19 diagnosis worldwide, therefore, must become accessible, harnessing inexpensive methods that can be carried out locally {[}7{]}.

An increasingly popular alternative to PCR is rapid antigen testing (RAT) {[}8{]}.
Like PCR, these tests have high specificity for COVID-19 while being less expensive, easier to implement, and faster but with lower sensitivity {[}9{]}.
For PCR testing, patients must travel to a designated site or have officials visit their home in enhanced personal protective equipment and results can take from one day to a week.
In contrast, RATs can be conducted on nasal swabs, completed at home with minimal PPE, and results are available in 30 minutes.
RATs can be taken by persons with limited training, thus decreasing the time and expense associated with identifying cases.
Together, these traits make RATs an appealing alternative to PCR, however, concerns have been raised that the lower sensitivity of RAT {[}10{]} leads to unacceptably many false negative diagnoses.

Another diagnostic that has been used since the start of the pandemic is symptom-thresholding {[}11{]}.
Here, clinicians determine a set of symptoms that they believe mean a patient has COVID-19, and treat patients with those symptoms (i.e.~to have a given number of those symptoms is to exceed the symptom threshold) as COVID-19 positive patients.
The main advantage of this approach is the ease of implementation.
As with RAT, symptom-thresholding is faster, cheaper and less invasive than PCR.
Unlike RAT, symptom-thresholding can be scaled immediately at the onset of a pandemic, however, it is also reliant on thresholds developed then.
These thresholds were necessarily drawn from clinical intuition, rather than data, often for different variants and populations.
Consequently, the relationship between the thresholds and the true COVID-19 status is often weak, necessarily including many non-COVID-19 diseases to catch any COVID-19 cases, meaning low specificity and thus a very large number of false positive diagnoses.
A natural extension, therefore, is syndromic modelling.
In this approach, rather than using a set of pre-determined thresholds, a range of symptomatic and risk factor data (such as age and gender ) are collected and then a sub-sample of patients is tested using PCR for validation {[}12{]}.
These data are used to fit a model that allows more accurate prediction of how likely a patient is to have COVID-19 through the identification of COVID-19 syndromes {[}14{]}.

It is worth highlighting at this point that in resource-limited settings there is very limited provision for testing of asymptomatic cases, despite their important role in disease transmission {[}15{]}.
Even while focusing solely on symptomatic patients, syndromic modelling is a complex and nuanced task.
Disease syndromes can change between populations, when new variants emerge, and as other diseases become more or less common {[}16{]}.
These changes can make syndromic models generalise poorly.
For example, if another disease for which loss of smell is a symptom becomes common, loss of smell is no longer strongly indicative of COVID-19.
Similarly, if everyone who presents has a cough, regardless of their COVID-19 status, then coughing will show no relationship with COVID-19 (even if the two are strongly related in the general population).
Furthermore, symptoms do not always occur in isolation, some, like loss of smell and loss of taste, are strongly related.
Unfortunately, the majority of syndromic modelling methods currently used do not account for these complexities.
Even where they can, at least partially, be accounted for, the many types of common respiratory disease generally means that syndromic modelling still tends to have quite low specificity {[}16{]}.

None of these alternative diagnostics, therefore, can match PCR testing in terms of both sensitivity and specificity.
However, this may be tolerable depending on the scale and impact of misclassification given the local situation.
Low specificity means a patient is likely to be told they have COVID-19 when they do not (a high false positive rate), leading to patients unnecessarily self-isolating and receiving support.
This is expensive to the individual and to local public health bodies, reducing available resources for those who need them {[}17{]}.
Similarly, low sensitivity means more patients being told they do not have COVID-19 when they actually do (a high false negative rate), leading to the individual not getting appropriate support or taking action to prevent the disease spreading further {[}18{]}.

Consider three epidemiological scenarios, which we will term the ``Agnostic,'' ``Low-Level Cases'' and ``Rising Cases'' scenarios.
The ``Agnostic'' scenario reflects the default, approach to minimise both misclassification rates, the true costs of these misclassifications will depend on local context.
When the prevalence of the disease is low, false negatives will be correspondingly low and false positives may create local scepticism leading to poor adherence longer term.
In this situation (our ``Low-Level Cases'' scenario), false positives might be more costly than false negatives {[}17{]}.
If the disease is abundant or increasing rapidly then changes in the false negative rate might have an outsized impact on the pandemic trajectory and thus be more costly, as in our ``Rising Cases'' scenario.
Often the situation will be even more nuanced and a different balance will need to be struck {[}5{]}.

The ``best'' diagnostic, therefore, is not a single universal test but the one where the correct classifications have highest value and misclassifications have lowest cost.
When not adapted to the local situation, the two dominant testing methods available in LMICs are highly flawed.
Relying solely on symptomatic diagnosis will likely overestimate the number of individuals with COVID-19 due to its lack of specificity.
Conversely, RATs will give a false impression of control due to the number of positive cases that will be missed.
In this paper, we demonstrate that by combining these two testing methods we can utilise their complementary strengths, ameliorate their respective weaknesses, and tune them for different epidemiological scenarios.
We compare the performance of these two testing methods and the combined approach both in terms of general prediction and as diagnostics under three epidemiological scenarios with different misclassification requirements.
We show that the optimised combined data models achieve equal-to-much-lower error rates than the next best method in all metrics.
We then discuss the role of statistically integrating data from multiple imperfect testing methods in resource limited settings to improve the diagnosis of diseases, particularly COVID-19.

\hypertarget{methods-950-words}{%
\section{Methods (950 words)}\label{methods-950-words}}

\hypertarget{data-collection}{%
\subsection{Data Collection}\label{data-collection}}

Participants included in this study were identified for COVID-19 testing by community support teams (CSTs).
Recruitment took place across Dhaka (the capital city of Bangladesh) between 19th May 2021 and 11th July 2021.

Patients were selected for further testing if they had a fever (\textgreater38°C) at the point of testing and one or more of 14 symptoms associated with COVID-19 (breathing problems, coughing, diarrhoea, fever (ongoing), a headache, loss of taste, loss of smell, muscle pain, red eyes, a runny nose, a sore throat, tiredness, vomiting or a wet cough).
If selected, the CSTs collected the patient's age and gender, and took two nasal swabs.

One swab each was used for rapid antigen testing (RAT) and reverse transcription polymerase chain reaction (RT-PCR, hereafter, PCR)
The full questionnaire and testing protocols are provided in Supplementary 1.
Participants provided written informed consent to sample collection and for their results to be analysed in the study.

\begin{figure}
\includegraphics[width=0.75\linewidth]{/Users/fergusjchadwick/Dropbox/Git/COVID19_SyndromicRATDiagnosis/0500_Manuscript/0501_MainText/MainTextFigs/DataFlowchart} \caption{Schematic description of identification of likely COVID-19 patients by community support teams (CSTs), swab collection and model definitions. The teams collected syndromic data (age, gender and presence/absence of 14 predetermined symptoms), and two sets of naso-pharyngeal swabs (one each for Rapid Antigen Testing and PCR). We then used rapid antigen testing (RAT) and the syndromic data, two imperfect but inexpensive diagnostics, to generate three model classes: RAT result only in Model Class 1, syndromic data only in Model Class 2, and both RAT result and syndromic data in Model Class 3. The PCR test result is used to train and test each model using temporal cross-validation.}\label{fig:data-flowchart}
\end{figure}

\hypertarget{modelling}{%
\subsection{Modelling}\label{modelling}}

\hypertarget{structure}{%
\subsubsection{Structure}\label{structure}}

We examined the ability of the two imperfect identification methods, syndromic modelling and RAT, to predict the patient's COVID-19 status when used separately and together.
These combinations define three model classes (Figure \ref{fig:data-flowchart}).

Model Class 1 uses only the RAT result.
It equates being RAT-positive with the patient being PCR-positive for COVID-19 (hereafter, PCR-positive), and being RAT-negative with PCR-negativity.

Model Class 2 uses only the syndromic data.
For this model, we used a Bayesian multivariate probit model {[}19{]}.
The multivariate probit structure allows the model to account for the binary and correlated nature of the symptoms while conditioning on the risk factors of age and gender.
By using a Bayesian formulation, we generate full posteriors for our parameter estimates, allowing natural quantification of uncertainty.

Model Class 3 combines the two data sources.
We utilise the specificity of RAT by treating RAT-positive patients as PCR-positive patients.
The RAT-negative patients are modelled using the sensitive syndromic approach using Model Class 2 to capture PCR-positive patients that are missed by the RAT.
This approach leverages the potential different syndromic profiles of PCR-positive patients who are RAT-positive and -negative, allowing the model to adapt solely to the latter.
The models were fitted to the data using Bayesian inference techniques based on Hamiltonian Monte Carlo in the Stan programming language {[}20{]}.

\hypertarget{model-selection}{%
\subsubsection{Model Selection}\label{model-selection}}

We conducted backwards model selection (starting with the most complex, biologically plausible model) to identify a subset of models with the highest predictive power under temporal cross-validation (Figure \ref{fig:modsel-flowchart}).
Reducing the number of possible models was necessary to reduce computational demand and reduce the risk of overfitting models to the test scenarios.
The large number of symptoms corresponds to a high number of potential model configurations (\textgreater131 000 for 14 symptoms and two covariates) which might perform well on the test sets (even under the challenging conditions of temporal cross-validation) but lack transferability.
By first using strength of relationship with the outcome (coarse selection) and general predictive power (fine selection) to narrow down the number of candidate models, and then testing those models in the scenarios, we are more likely to choose models which generalise well to new data.
During the fitting process, it was clear that there were ``jumps'' in performance (as defined below) between models containing five and four symptoms, so the models with one to four symptoms were used as the candidate models.
Zero symptom models were not included in the analysis as they do not correspond to a feasible policy.

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/fergusjchadwick/Dropbox/Git/COVID19_SyndromicRATDiagnosis/0500_Manuscript/0501_MainText/MainTextFigs/ModelSelectionFlowchart} \caption{Schematic for rounds of model selection in the multivariate probit component of Model Classes 2 and 3. With 14 symptoms (only 5 shown here for demonstration purposes) and two covariates there are over 131000 possible model combinations. To make exploring these possible models computationally feasible and to reduce the risk of overfitting, we carried out two rounds of model selection. First, the data are divided into temporal cross-validation sets. The multivariate probit connects symptoms to the PCR result through a correlation matrix. In the coarse model selection, the most complex feasible model (all symptoms and covariates) is fit to the training data. The estimated correlations between each symptom and the PCR result are compared for each cross-validation set. The symptoms that have non-zero correlations in a systematic direction (i.e. all positively or all negatively correlated with PCR result) are retained. The process is then repeated on each retained set of symptoms until the four symptoms in each model class with the strongest correlation to PCR result. We then conduct a more exhaustive model selection on all the possible permutations of the four symptoms and two covariates. In this round, each model is fit to training data and used to predict for the test set, and the quality of those predictions is measured using cross-entropy scoring. The cross-entropy score is then used to select the best predictive model for each level of model complexity. Only these final models are then used for classification. This reduces the set of models tested as classifiers from >131 000 to just four per model class.}\label{fig:modsel-flowchart}
\end{figure}

\hypertarget{predictive-performance}{%
\subsubsection{Predictive Performance}\label{predictive-performance}}

We scored the models' predictive power using cross-entropy.
Cross-entropy measures the accuracy of models that generate probabilities of binary outcomes, rather than make binary classifications, similar in concept to a mean square error for normally-distributed data, but adapted for binary data {[}21{]}.
A cross-entropy value close to zero corresponds to high levels of accuracy, with larger values indicating lower accuracy.
More details on the model structure and selection process, including code, are available in Supplementary 2.

\hypertarget{classification-performance}{%
\subsubsection{Classification Performance}\label{classification-performance}}

In applied settings, models must often be evaluated on their performance as classifiers rather than just as prediction engines (i.e.~their ability to say a patient is COVID-19 positive or negative, not simply the probability the patient might be COVID-19 positive or negative).
To generate a classification, a probability threshold must be chosen over which patients are classified as COVID-19 positive.

Classifier performance was compared both generically (using receiver operating characteristic (ROC) curves to look at the error rates that can be acheived with each model without specifying a scenario {[}22{]}) and under three epidemiological scenarios (using error terms described in Table \ref{tab:scenarios-tab}).
We strongly emphasise that generic performance here is only used to show the flexibility of the model classes; the best model for a local situation can only be determined if the relative cost of false positives and false negatives is known.

\begin{table}

\caption{\label{tab:scenarios-tab}For each epidemiological scenario there is a requirement and a performance criterion.
The requirement refers to a base level of performance the model must achieve; in general this will be a maximum acceptable error rate of some kind.
These requirements were determined in discussion with members of the Institute of Epidemiology, Disease Control and Research, Ministry of Health, Bangladesh (IEDCR).
The requirement determines a probability threshold for each model which most closely exceeds that requirement (i.e. for a 20% requirement, 20.1% error is unacceptable even though it might be the closest achieve error rate to the requirement).
The performance criterion is then used to determine which model performs the 'best' given that the requirement has been met.}
\centering
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
Scenario Name & Requirement & Performance Criterion (Error)\\
\midrule
1 Agnostic & Maximise correct classification rates & Sum of error rates\\
2 Rising Cases & 20\% false negative rate & False positive rate\\
3 Low-Level Cases & 20\% false positive rate & False negative rate\\
\bottomrule
\end{tabu}
\end{table}

In Scenario 1, we do not consider epidemiological context but simply minimise false negative and false positive rates equally.
We do this by maximising the two correct classification rates both individually and in total, as measured by the harmonic mean (as opposed to the arithmetic mean which would only maximise the rates in total).
Scenario 2 corresponds to the current situation in Bangladesh at time of writing (July 2021), with COVID-19 cases beginning to rapidly increase again.
Under these circumstances, false negatives are extremely costly relative to false positives due to the exponential growth of the disease.
In Scenario 3, the pandemic is not declining but maintaining a steady rate of cases.
In this situation, policy-makers may be keen to keep false positive diagnoses low to prevent lockdown fatigue and to keep the workforce active.
The requirements in Scenario 2 and 3 were developed in discussion with the Institute of Epidemiology, Disease Control and Research (IEDCR), Bangladesh, for illustrative purposes.

\hypertarget{results-550-words}{%
\section{Results (550 words)}\label{results-550-words}}

Of 1241 subjects surveyed, a total of \text{1172} subjects had complete data available for the current analyses with the remainder removed due to duplication of barcodes or missing data.
The mean age of women participants (\text{47}\% of the sample) was \text{37} (SD = \text{14}) years, and for men (\text{53}\% of the sample) was \text{36} (SD = \text{14}) years.
Participants were identified by the community support teams (CSTs) and drawn from across Dhaka.

Model selection for both Model Class 2 (syndromic data only) and 3 (syndromic and RAT data) showed a marked decline in predictive power at more than 4 symptoms.
The covariate gender was dropped for both model classes while age was dropped in Class 2 but retained in Class 3.
The final four symptoms in order of importance (i.e.~the most important symptom was retained in all of the final 4 models, the least important symptom was only retained in the 4 symptom model) were loss of taste, diarrhoea, vomit and fever for Model Class 2, and fever, wet cough, cough and loss of taste for Model Class 3.

In the comparison of model predictive performance, Model Class 1 (RAT only) performed worst with an out-of-sample cross-entropy of \text{3·24} (cross-entropy values further from zero correspond to worse predictive performance).
The median cross-entropy values were between \text{2·53} and \text{2·59} for models in Class 2.
Models in Class 3 performed best with cross-entropy values between \text{1·44} and \text{1·47} (see Figure \ref{fig:pred-perf}).

\begin{figure}
\includegraphics[width=1\linewidth]{0501_MainText_files/figure-latex/pred-perf-1} \caption{Predictive performance of candidate models. Interquartile ranges for the posterior cross-entropy of the best candidate models at each level of model complexity tested under temporal cross-validation. cross-entropy is a measure of distance from the truth, so values closer to zero indicate better models. The intermediate complexity models perform best at prediction, although performance is similar across all the models within each model class (1: rapid antigen testing (RAT) only; 2: syndromic data only; and 3: combined RAT and syndromic data).}\label{fig:pred-perf}
\end{figure}

Generic model classification performance is shown by their ROC curves (Figure \ref{fig:ROC-plot}).

\begin{figure}
\includegraphics[width=1\linewidth]{0501_MainText_files/figure-latex/ROC-plot-1} \caption{Receiver operating characteristics for rapid antigen testing (RAT) only approach (Model Class 1) and posterior median and interquartile range ROC for Class 2 (syndromic data only) and 3 (syndromic and RAT data) models. These curves demonstrate the performance of the model for any hypothetical scenario as defined by the axes (as opposed to Figure 5 which demonstrates model performance in specific epidemiological scenarios which are realisations of a single point in this space).}\label{fig:ROC-plot}
\end{figure}

\begin{figure}
\includegraphics[width=1\linewidth]{0501_MainText_files/figure-latex/scenario-plot-1} \caption{Performance of models under each scenario measured by posterior median and interquartile range for errors defined in Table 1. Low errors correspond to better model performance. There is no error rate defined for the Model Class 1 (RAT only model) in Scenario 2 as the model failed to meet the requirement for that scenario (making the error functionally infinite).}\label{fig:scenario-plot}
\end{figure}

Scenario specific classification performance is shown in Figure \ref{fig:scenario-plot}.
Across all three scenarios (defined in Table \ref{tab:scenarios-tab}), the best models in Class 3 performed equally well or better than the other two model classes.
In Scenario 1 (``Agnostic''), models in Classes 1 and 3 performed equally well (overlapping posterior interquartile ranges) and distinctly better (no overlap in posterior interquartile range) than models in Class 2.
The median error was \text{0·47} for models in Class 1 and Class 3 and between \text{0·87} and \text{0·9} for models in Class 2 (Figure \ref{fig:scenario-plot}).
In Scenario 2 (``Rising Cases''), Model Class 1 failed to meet the requirement and so was excluded, and Model Class 3 once again outperformed Class 2.
The median errors were between \text{0·75} and \text{0·76} for models in Class 2, and \text{0·44} and \text{0·49} for models in Class 3 (Figure \ref{fig:scenario-plot}).
In Scenario 3 (``Low-Level Cases''), Model Class 2 once again performed worst, and Model Class 3 achieved the lowest error, with Model Class 1 falling in between the two (closer to Class 3 than 2).
The error in Class 1 was \text{0·02} and the median errors ranged from \text{0·19} to \text{0·2} for Class 2, and \text{0·18} to \text{0·2} for Class 3 (Figure \ref{fig:scenario-plot}).
For Classes 2 and 3 across all the scenarios the number of symptoms made relatively little difference within the final four candidate models in terms of median performance, although the more complex models have higher precision.
It should be noted that the candidate models are chosen as a result of a selection process and performed much better than more complex models (i.e.~those with 5 or more symptoms) or simpler models (with no symptoms but an intercept and covariates) in terms of cross-entropy and ROC.

\hypertarget{discussion-816-words}{%
\section{Discussion (816 words)}\label{discussion-816-words}}

We have demonstrated that combining rapid antigen tests (RATs) with syndromic modelling yields better identification of COVID-19 cases than either diagnostic in isolation.
These gains in performance are mirrored across metrics of prediction, general classification and scenario-specific classification.
The biggest improvement is seen in Scenario 2 (``Rising Cases'') which was developed around the current situation in Bangladesh (see Table \ref{tab:scenarios-tab} where the pandemic is once again accelerating, a trend mirrored in many low- and middle- income countries (LMICs).
In this scenario, the combined data model (Model Class 3) false negative rate is \text{26} (IQR: \text{24}-\text{29}) percentage points lower that of the RAT only model (Model Class 1).
Although the syndromic only model (Model Class 2) matches the combined model's false negative rate, its false positive rate is \text{31} (IQR: \text{29}- \text{34}) percentage points higher.

In a country where there are currently 15 000 new cases being identified every day, these improvements are non-trivial, representing tens of thousands of daily cases that would otherwise be missed.
Furthermore, this boost in diagnostic performance is achieved with data that are either already being collected or are currently being rolled out in Bangladesh and other LMICs {[}24{]}.
The only cost involved in making these improvements is the model development which is easily scalable.

The pattern is similar in epidemiological Scenarios 1 (``Agnostic'') and 3 (``Low-Level Cases''), with the combined model class performing equally well or better than the other two classes (Figure \ref{fig:scenario-plot}).
These three scenarios only offer snapshots of performance.
An indication of how these models will perform under any condition can be obtained by comparing the more generic model performance metrics for prediction and classification (Figures \ref{fig:pred-perf} and \ref{fig:ROC-plot}, respectively).
These figures demonstrate both the added flexibility of the more complex model classes that allow them to be tailored to specific needs and the need to combine the high-quality but inflexible RAT results with the more flexible but lower quality syndromic data.

The final symptoms chosen through model selection should be interpreted cautiously.
These models were developed for prediction and classification in a unique sub-population: CST-identified, symptomatic patients.
Different symptoms and risk factors were retained for different model classes, despite these data being collected over a short time period from the same population.
These differences may point to mechanisms by which CST-identified and RAT-positive patients differ from other groups.
Of particular interest is whether individuals that are identified by reverse transcription polymerase chain reaction (RT-PCR, hereafter, PCR) but missed by RAT are less infectious and thus more typical of the asymptomatic population (possibly with some symptomatic co-morbidity).
This could be explored by using viral load measured as Threshold Cycle (Ct) values from the PCR {[}25{]} and further testing for other illnesses.

Our methodology has been developed using a large sample size drawn under field-realistic conditions and has thus developed with the practicalities of mass deployment in mind.
Improving case identification using statistical methods allows us to update our diagnostic process in real-time, allowing rapid adaptation to new variants or even new diseases.
The modelling frameworks we have used are also sufficiently flexible to accommodate new data sources (such as background case numbers) or changes in the local relative costs of false positives and false negatives.

Naturally, these strengths have complementary limitations.
Our models can only achieve good performance if the validation data are of good quality.
Similarly, targeting misdiagnosis rates is only sensible if those rates properly reflect local conditions which can be challenging.
While these limitations should be seriously considered, we believe that the alternatives simply hide these problems.
We choose to make these decisions explicitly to allow them to be more readily challenged, researched and improved upon.
These challenges represent promising new avenues for impactful research that improve our understanding of estimating misdiagnosis rate trade offs and how to translate sample population findings to target populations.

We believe that combined syndromic and rapid antigen testing approach is the most promising method for large-scale testing in LMICs for COVID-19 at present.
We have demonstrated that these improvements can be impressive in real-world scenarios, and will have a large impact when scaled to the population sizes in LMICs.
The framework we outline above is adaptable for other diagnostic problems.
Malaria, schistosomiasis, rabies and many other diseases are all currently monitored either sparsely with gold-standard methods (such as PCR, autopsies, fluorescent antibody testing) or at a large scale with more error-prone methods (RATs, blood smears, egg counts, differential diagnosis).

The management of global pandemics can only be done with testing at scale.
While the quest to achieve this using only gold-standard diagnostic methods is laudable, it is also often impractical.
Imperfect diagnostics are frequently imperfect in different ways, and these differences are ripe for statistical treatment.
What is more, these approaches are often more agile than gold-standard diagnostics in situations of flux, for example, in the early stages of new pandemics or disease strains, when fast responses are essential.\\
By investing in understanding how to utilise the complementary strengths of imperfect testing and deploy the limited gold-standard testing available for validation, we can provide good quality testing at the scale needed to fight infectious diseases.

\hypertarget{funding-28-words}{%
\section{Funding (28 words)}\label{funding-28-words}}

The Bill and Melinda Gates Foundation funded work by FAO (INV-022851), University of Glasgow reports funding from the Wellcome Trust (207569/Z/17/Z) and EPSRC (EP/R513222/1).
The authors declare no competing interests.

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

We would like to thank members of the community support teams in Bangladesh who have provided essential services throughout the pandemic.
Earlier drafts of this manuscript benefited from the input of Paul Johnson, Daniel Haydon, Frances Mair, Anne-Sophie Bonnet-Lebrun, Luca Nelli, Crinan Jarrett, Rita Claudia Cardoso Ribeiro, Halfan Ngowo, Heather McDevitt and Gina Bertolacci.
The University of Glasgow COVID-19 in LMICs Group provided the environment in which to develop this work.

\hypertarget{references-max-30}{%
\section*{References (Max 30)}\label{references-max-30}}
\addcontentsline{toc}{section}{References (Max 30)}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\hypertarget{ref-drame2020should}{}%
\CSLLeftMargin{{[}1{]} }
\CSLRightInline{Dramé M, Teguo MT, Proye E, Hequet F, Hentzien M, Kanagaratnam L, et al. Should RT-PCR be considered a gold standard in the diagnosis of covid-19? Journal of Medical Virology 2020.}

\leavevmode\hypertarget{ref-corman2020detection}{}%
\CSLLeftMargin{{[}2{]} }
\CSLRightInline{Corman VM, Landt O, Kaiser M, Molenkamp R, Meijer A, Chu DK, et al. Detection of 2019 novel coronavirus (2019-nCoV) by real-time RT-PCR. Eurosurveillance 2020;25:2000045.}

\leavevmode\hypertarget{ref-tahamtan2020real}{}%
\CSLLeftMargin{{[}3{]} }
\CSLRightInline{Tahamtan A, Ardebili A. Real-time RT-PCR in COVID-19 detection: Issues affecting the results. Expert Review of Molecular Diagnostics 2020;20:453--4.}

\leavevmode\hypertarget{ref-chowdhury2020long}{}%
\CSLLeftMargin{{[}4{]} }
\CSLRightInline{Chowdhury R, Luhar S, Khan N, Choudhury SR, Matin I, Franco OH. Long-term strategies to control COVID-19 in low and middle-income countries: An options overview of community-based, non-pharmacological interventions. European Journal of Epidemiology 2020;35:743--8.}

\leavevmode\hypertarget{ref-vandenberg2021considerations}{}%
\CSLLeftMargin{{[}5{]} }
\CSLRightInline{Vandenberg O, Martiny D, Rochas O, Belkum A van, Kozlakidis Z. Considerations for diagnostic COVID-19 tests. Nature Reviews Microbiology 2021;19:171--83.}

\leavevmode\hypertarget{ref-cash2020has}{}%
\CSLLeftMargin{{[}6{]} }
\CSLRightInline{Cash R, Patel V. Has COVID-19 subverted global health? The Lancet 2020;395:1687--8.}

\leavevmode\hypertarget{ref-olalekan2020covid}{}%
\CSLLeftMargin{{[}7{]} }
\CSLRightInline{Olalekan A, Iwalokun B, Akinloye OM, Popoola O, Samuel TA, Akinloye O. COVID-19 rapid diagnostic test could contain transmission in low-and middle-income countries. African Journal of Laboratory Medicine 2020;9:1--8.}

\leavevmode\hypertarget{ref-linares2020panbio}{}%
\CSLLeftMargin{{[}8{]} }
\CSLRightInline{Linares M, Pérez-Tanoira R, Carrero A, Romanyk J, Pérez-García F, Gómez-Herruz P, et al. Panbio antigen rapid test is reliable to diagnose SARS-CoV-2 infection in the first 7 days after the onset of symptoms. Journal of Clinical Virology 2020;133:104659.}

\leavevmode\hypertarget{ref-boum2021performance}{}%
\CSLLeftMargin{{[}9{]} }
\CSLRightInline{Boum Y, Fai KN, Nikolay B, Mboringong AB, Bebell LM, Ndifon M, et al. Performance and operational feasibility of antigen and antibody rapid diagnostic tests for COVID-19 in symptomatic and asymptomatic patients in cameroon: A clinical, prospective, diagnostic accuracy study. The Lancet Infectious Diseases 2021.}

\leavevmode\hypertarget{ref-mak2020evaluation}{}%
\CSLLeftMargin{{[}10{]} }
\CSLRightInline{Mak GC, Cheng PK, Lau SS, Wong KK, Lau C, Lam ET, et al. Evaluation of rapid antigen test for detection of SARS-CoV-2 virus. Journal of Clinical Virology 2020;129:104500.}

\leavevmode\hypertarget{ref-jin2020rapid}{}%
\CSLLeftMargin{{[}11{]} }
\CSLRightInline{Jin Y-H, Cai L, Cheng Z-S, Cheng H, Deng T, Fan Y-P, et al. A rapid advice guideline for the diagnosis and treatment of 2019 novel coronavirus (2019-nCoV) infected pneumonia (standard version). Military Medical Research 2020;7:1--23.}

\leavevmode\hypertarget{ref-sim2021utilizing}{}%
\CSLLeftMargin{{[}12{]} }
\CSLRightInline{Sim JXY, Conceicao EP, Wee LE, Aung MK, Seow SYW, Teo RCY, et al. Utilizing the electronic health records to create a syndromic staff surveillance system during the COVID-19 outbreak. American Journal of Infection Control 2021;49:685--9.}

\leavevmode\hypertarget{ref-undurraga2021covid}{}%
\CSLLeftMargin{{[}13{]} }
\CSLRightInline{Undurraga EA, Chowell G, Mizumoto K. COVID-19 case fatality risk by age and gender in a high testing setting in latin america: Chile, march--august 2020. Infectious Diseases of Poverty 2021;10:1--1.}

\leavevmode\hypertarget{ref-wenham2020covid}{}%
\CSLLeftMargin{{[}14{]} }
\CSLRightInline{Wenham C, Smith J, Morgan R. COVID-19: The gendered impacts of the outbreak. The Lancet 2020;395:846--8.}

\leavevmode\hypertarget{ref-mayorga2020modelling}{}%
\CSLLeftMargin{{[}15{]} }
\CSLRightInline{Mayorga L, Samartino CG, Flores G, Masuelli S, Sánchez MV, Mayorga LS, et al. A modelling study highlights the power of detecting and isolating asymptomatic or very mildly affected individuals for COVID-19 epidemic management. BMC Public Health 2020;20:1--1.}

\leavevmode\hypertarget{ref-garry2020considerations}{}%
\CSLLeftMargin{{[}16{]} }
\CSLRightInline{Garry S, Abdelmagid N, Baxter L, Roberts N, Waroux O le P de, Ismail S, et al. Considerations for planning COVID-19 treatment services in humanitarian responses. Conflict and Health 2020;14:1--1.}

\leavevmode\hypertarget{ref-surkova2020false}{}%
\CSLLeftMargin{{[}17{]} }
\CSLRightInline{Surkova E, Nikolayevskyy V, Drobniewski F. False-positive COVID-19 results: Hidden problems and costs. The Lancet Respiratory Medicine 2020;8:1167--8.}

\leavevmode\hypertarget{ref-west2020covid}{}%
\CSLLeftMargin{{[}18{]} }
\CSLRightInline{West CP, Montori VM, Sampathkumar P. COVID-19 testing: The threat of false-negative results. Mayo clinic proceedings, vol. 95, Elsevier; 2020, p. 1127--9.}

\leavevmode\hypertarget{ref-albert1993bayesian}{}%
\CSLLeftMargin{{[}19{]} }
\CSLRightInline{Albert JH, Chib S. Bayesian analysis of binary and polychotomous response data. Journal of the American Statistical Association 1993;88:669--79.}

\leavevmode\hypertarget{ref-carpenter2017stan}{}%
\CSLLeftMargin{{[}20{]} }
\CSLRightInline{Carpenter B, Gelman A, Hoffman MD, Lee D, Goodrich B, Betancourt M, et al. Stan: A probabilistic programming language. Journal of Statistical Software 2017;76:1--32.}

\leavevmode\hypertarget{ref-gneiting2007strictly}{}%
\CSLLeftMargin{{[}21{]} }
\CSLRightInline{Gneiting T, Raftery AE. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association 2007;102:359--78.}

\leavevmode\hypertarget{ref-hoo2017roc}{}%
\CSLLeftMargin{{[}22{]} }
\CSLRightInline{Hoo ZH, Candlish J, Teare D. What is an ROC curve? 2017.}

\leavevmode\hypertarget{ref-aziz2020integrated}{}%
\CSLLeftMargin{{[}23{]} }
\CSLRightInline{Aziz AB, Raqib R, Khan WA, Rahman M, Haque R, Alam M, et al. Integrated control of COVID-19 in resource poor countries 2020.}

\leavevmode\hypertarget{ref-schultz2021pragmatic}{}%
\CSLLeftMargin{{[}24{]} }
\CSLRightInline{Schultz MJ, Gebremariam TH, Park C, Pisani L, Sivakorn C, Taran S, et al. Pragmatic recommendations for the use of diagnostic testing and prognostic models in hospitalized patients with severe COVID-19 in low-and middle-income countries. The American Journal of Tropical Medicine and Hygiene 2021;104:34.}

\leavevmode\hypertarget{ref-albert2021field}{}%
\CSLLeftMargin{{[}25{]} }
\CSLRightInline{Albert E, Torres I, Bueno F, Huntley D, Molla E, Fernández-Fuentes MÁ, et al. Field evaluation of a rapid antigen test (panbio™ COVID-19 ag rapid test device) for COVID-19 diagnosis in primary healthcare centres. Clinical Microbiology and Infection 2021;27:472--e7.}

\end{CSLReferences}


\end{document}
